{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies \n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url pages to be scraped and used \n",
    "mars_nasa = 'https://mars.nasa.gov/news/'\n",
    "mars_image = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "mars_weather = 'https://twitter.com/marswxreport?lang=en'\n",
    "mars_facts = 'http://space-facts.com/mars/'\n",
    "mars_hemispheres = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use requests module to get information from mars_nasa\n",
    "response_mars_nasa = requests.get(mars_nasa)\n",
    "# use beautiful soup, create object and parse with html.parser\n",
    "soup = BeautifulSoup(response_mars_nasa.text, 'html.parser')\n",
    "# display content to show what there is to scrape\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results are returned as an iterable list\n",
    "results = soup.find_all(class_=\"slide\")\n",
    "list_titles = []\n",
    "list_pp = []\n",
    "# loop through all the returned results \n",
    "for result in results:\n",
    "    # handling errors\n",
    "    try:\n",
    "        #find the title and pp for each of the links. the title is found in the second link of each slide, while pp is inside the div tag description\n",
    "        links = result.find_all('a')\n",
    "        title = links[1].text\n",
    "        paragraph = result.find(class_=\"rollover_description_inner\").text\n",
    "        # append each variable to predetermined lists\n",
    "        list_titles.append(title)\n",
    "        list_pp.append(paragraph)\n",
    "        \n",
    "        print(title)\n",
    "        print(paragraph)\n",
    "    except AttributeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the first title and body \n",
    "first_title = list_titles[0]\n",
    "first_pp = list_pp[0]\n",
    "print(first_title)\n",
    "print(first_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape for mars image and retreive page with requests module\n",
    "image_pull = requests.get(mars_image)\n",
    "# use beautiful soup, create object and parse with html.parser\n",
    "soup = BeautifulSoup(image_pull.text, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return results as iterable list\n",
    "results = soup.find_all(class_=\"carousel_items\")\n",
    "# loop through results \n",
    "for result in results:\n",
    "    # handle erroes\n",
    "    try:\n",
    "        #query article tag and distinguish it as a 'style' parameter \n",
    "        article = result.find('article', class_=\"carousel_item\")\n",
    "        link_article = article['style']\n",
    "        #Use modification to fix the link to be in the correct format\n",
    "        cleaned_link_article = article['style'].lstrip('background-image: url(')\n",
    "        cleaned_link_article = cleaned_link_article.rstrip(');')\n",
    "        # show the link, the link once cleaned, and the article it was taken from \n",
    "        print(link_article)\n",
    "        print(cleaned_link_article)\n",
    "        print(article)\n",
    "    except AttributeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove quotes from beginning and end of the string and then create the url for the image \n",
    "cleaned_link_article = cleaned_link_article.replace(\"'\", \"\")\n",
    "featured_image_link = 'https://www.jpl.nasa.gov'+cleaned_link_article\n",
    "print(featured_image_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the page with the requests module \n",
    "weather_response = requests.get(mars_weather)\n",
    "# create B soup object and parse with html.parser\n",
    "soup = BeautifulSoup(weather_response.text, 'html.parser')\n",
    "# look at the results\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results are returned as an iterable list\n",
    "results = soup.find_all(class_=\"content\")\n",
    "list_tweets = []\n",
    "# loop through the returned results \n",
    "for result in results:\n",
    "    # handle errors\n",
    "    try:\n",
    "        # find the test of the tweets and append it to the tweet text \n",
    "        tweet = result.find('p', class_=\"TweetTextSize\").text\n",
    "        list_tweets.append(tweet)\n",
    "    except AttributeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  store the most recent tweets as first tweets that are added to the list and you can always modify the index to return the first weather entry as a way to check the recent updates \n",
    "mars_weather = list_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to scrape the data \n",
    "table_of_facts = pd.read_html(mars_facts)\n",
    "table_of_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new dataframe that pulls the information from table of tacts and turns it into a dictionary \n",
    "keys = list(table_of_facts[0][0])\n",
    "values = list(table_of_facts[0][1])\n",
    "facts_dict = dict((keys[x],values[x]) for x in range(0,len(keys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns that are not needed and show the respective df \n",
    "facts_df = pd.DataFrame(facts_dict, index=[0])\n",
    "facts_df = facts_df.drop(['First Record:', 'Recorded By:'], axis=1)\n",
    "facts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns\n",
    "cols = facts_df.columns.tolist()\n",
    "new_columns = ['Equatorial Diameter:', 'Polar Diameter:', 'Mass:', 'Orbit Distance:', 'Orbit Period:', 'Surface Temperature:', 'Moons:']\n",
    "facts_df = facts_df[new_columns]\n",
    "facts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the Chrome Driver \n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base url and the lists that hold all the necessary links \n",
    "usgs_base= 'https://astrogeology.usgs.gov'\n",
    "links_list = []\n",
    "hemispheres_response = requests.get(mars_hemispheres)\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "soup = BeautifulSoup(hemispheres_response.text, 'html.parser')\n",
    "\n",
    "results = soup.find_all(class_='item')\n",
    "# loop through all the results \n",
    "for result in results:\n",
    "    # handle the errors \n",
    "    try:\n",
    "        # find the tag which will link you to a speciic page \n",
    "        links = result.find('a')\n",
    "        # return the link and print it \n",
    "        link=links['href']\n",
    "        print(link)\n",
    "       \n",
    "        # add in the usgs url to complete the full url and add that to a list of the finished links \n",
    "        links_list.append(usgs_base+link)\n",
    "    except AttributeError as e:\n",
    "        \n",
    "print(links_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use spliinter to tap into the hemispheres url \n",
    "browser.visit(mars_hemispheres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the lists that hold all of the urls and their respective titles \n",
    "hemispheres_image = []\n",
    "first_title = []\n",
    "\n",
    "for x in range(0, 4):\n",
    "    # navigate through all the four page links \n",
    "    browser.visit(links_list[x])\n",
    "\n",
    "    # print each of the links as they are being processed \n",
    "    print(links_list[x])\n",
    "\n",
    "    # create soup object \n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # look for the jpg that we want in the downloads class \n",
    "    images = soup.find(class_='downloads')\n",
    "\n",
    "    # search for the link tag which then retrieves the desired link \n",
    "    image = images.find('a')\n",
    "    image_url= image['href']\n",
    "\n",
    "    # append that to the hemispheres_image list\n",
    "    hemispheres_image.append(image_url)\n",
    "\n",
    "    # searcg for the title and append that to the list of titles \n",
    "    titles = soup.find('h2', class_='title')\n",
    "    title=titles.text\n",
    "    title=title.strip('Enhanced')\n",
    "    titles_list.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for hemisheres that holds the title and the url \n",
    "hemispheres_dict = {'Title': titles_list,\n",
    "                    'URL': hemispheres_image }\n",
    "\n",
    "                    # create a dictionary with all the scraped data\n",
    "scraped_dict = {'Title': titles_list,\n",
    "                'URL': hemispheres_image,git init\n",
    "                'Weather': mars_weather,\n",
    "                'Featured Image': featured_image_link,\n",
    "                'News Title': first_title,\n",
    "                'News Body': news_p\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
